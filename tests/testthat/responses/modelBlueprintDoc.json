[{"task": "One Hot encoding task", "description": "This transformer will do a binary one-hot (aka one-of-K) coding. One boolean-valued feature is constructed for each of the possible string values that the feature can take. For inputs with only 2 unique values, only one boolean-valued feature will be constructed\nThis encoding is needed for feeding categorical data to many estimators, notably linear models and SVMs.", "parameters": [{"type": "int (default=‘5’)", "name": "min_support (sc)", "description": " The minimum number of records for a category to be represented in one hot encoding. If a category has fewer counts it will be grouped with other small cardinality values values: [1, 99999] "}, {"type": "int (default=‘1’)", "name": "card_min (cm)", "description": " An integer that specifies the minimum number of unique values values: [1, 99999] "}, {"type": "integer (default=‘100’)", "name": "card_max (cm)", "description": " An integer that specifies the maximum number of unique values values: [1, 99999] "}, {"type": "bool (default=’False’)", "name": "drop_cols (dc)", "description": " drop_cols, If True, drop last level of each feature values: [False, True] "}, {"type": "integer (default=’None’)", "name": "max_features (mf)", "description": " If the total number of categories created across all features exceeds this value, the top max_features most frequent categories will persist. All others will be either thrown out or grouped. A value of None disables the limit. values: [1, 999999] "}, {"type": "select (default=’None’)", "name": "flag (flag)", "description": " flag, If all, add highcat-cols to metadata values: ['None', 'all'] "}], "title": "One-Hot Encoding Documentation — DataRobot Model Documentation", "links": [{"url": "http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-categorical-features", "name": "sklearn preprocessing categorical features user guide"}, {"url": "http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-categorical-features", "name": "sklearn preprocessing categorical features user guide"}], "references": [{"url": "http://www.jstor.org/stable/2281705?seq=1", "name": "[R20]Suits, Daniel B. “Use of dummy variables in regression equations.” Journal of the American Statistical Association 52.280 (1957): 548-551. "}]}, {"task": "Standardize task", "description": "Standardize features by removing the median and scaling to unit standard deviation or the mean absolute deviation\nCentering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set.\nStandardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual feature do not more or less look like standard normally distributed data.\nFor instance many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the L1 and L2 regularizers of linear models) assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\nMarquardt, Donald W. “Comment: You should standardize the predictor variables in your regression models.” Journal of the American Statistical Association 75.369 (1980): 87-91.", "parameters": [{"type": "string (default=’std’)", "name": "scale_type (st)", "description": " Whether standard deviation or mean absolute deviation should be used values: ['std','mad'] "}, {"type": "float (default=0.25)", "name": "sparsity_threshold (st)", "description": " if sparsity level is higher than the parameter, matrix is converted to a sparse format values: [0, 1] "}], "title": "Standardize Documentation — DataRobot Model Documentation", "links": [], "references": []}, {"task": "Logistic Regression with low penalty based on block coordinate descent", "description": "Logistic is a class of generalized linear models that uses the binomial distribution to fit regression models to a binary (0/1) response variable. It is probably the most widely used binary classification models and is a good baseline by which to judge the performance of other classifiers. Logistic regression is notable in that it tends to produce well-calibrated classificiation probabilities without the need for post-processing.\nThe Elastic Net is an extension of logistic regression where the optimizer makes an attempt to find a parsimonious model, by having a preference for simpler models. A simpler model is defined as having coefficients with smaller absolute values as well as fewer non-zero coefficients.  This can help the model deal with co-linear variables, and can also produce models that are less prone to overfitting and generalize better to new data.  This “preference for simpler models” is formally definied as “regularization,” and the degree of regularization for non zero coefficients as well as the absolute value of the size of the coefficients are the 2 major meta parameters that control the model.\nElasticNet is a linear regression model trained with L1 and L2 prior as regularizer. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge.\nElastic-net is useful when there are multiple features which are correlated with one another. Lasso is likely to pick one of these at random, while elastic-net is likely to pick both.\nA practical advantage of trading-off between Lasso and Ridge is it allows Elastic-Net to inherit some of Ridge’s stability under rotation.\nBased on lightning CDClassifier", "parameters": [{"type": "bool (default=’True’)", "name": "fit_intercept (fi)", "description": " whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). values: [False, True] "}, {"type": "int (default=‘100’)", "name": "max_iter (mi)", "description": " The maximum number of iterations values: [1, 1e6] "}, {"type": "float (default=‘0.0001’)", "name": "tol (e)", "description": " The tolerance for the optimization: if the updates are smaller than tol, the optimization code checks the dual gap for optimality and continues until it is smaller than tol. values: [1e-10, 1e10] "}, {"type": "bool (default=’False’)", "name": "warm_start (ws)", "description": " When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. values: [False, True] "}, {"type": "int (default=‘1234’)", "name": "random_state (rs)", "description": " The seed of the pseudo random number generator to use. values: [0, int(1e9)] "}, {"type": "float (default=‘1e-6’)", "name": "sigma (sg)", "description": " Constant used in the line search sufficient decrease condition. values': [0, 1e-6] "}, {"type": "select (default=’id’)", "name": "beta_transform (bt)", "description": " beta_transform is a parameter used for blenders. If beta_transform is set to ‘blender’, coefficients are non-negative and are all in [0, 1]. Very large weight of the penalty term will get you the average blender. values: ['id', 'blender'] "}], "title": "Logistic Regression Documentation — DataRobot Model Documentation", "links": [{"url": "http://contrib.scikit-learn.org/lightning/generated/lightning.classification.CDClassifier.html", "name": "lightning classification CDClassifier"}, {"url": "http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression", "name": "sklearn Logistic Regression user guide"}, {"url": "http://scikit-learn.org/stable/modules/linear_model.html", "name": "sklearn Linear model user guide"}, {"url": "https://en.wikipedia.org/wiki/Logistic_regression", "name": "Logistic Regression wikipedia"}], "references": [{"url": "http://www.mblondel.org/publications/mblondel-mlj2013.pdf", "name": "[R36]Blondel, Mathieu, Kazuhiro Seki, and Kuniaki Uehara. “Block coordinate descent algorithms for large-scale sparse multiclass classification.” Machine Learning 93.1 (2013): 31-52. "}, {"url": null, "name": "[R37]Hosmer Jr, David W., and Stanley Lemeshow. “Applied logistic regression” John Wiley & Sons, 2004."}, {"url": "http://biostat.mc.vanderbilt.edu/tmp/course.pdf", "name": "[R38]Harrell, Frank E. “Regression modeling strategies: with applications to linear models, logistic regression, and survival analysis.” Springer Science & Business Media, 2013. "}]}, {"task": "Numeric impute task", "description": "Impute missing values on numeric variables with their median and create indicator variables to identify records that were imputed. Implements the fit_transform / transform API similar to sklearn", "parameters": [{"type": "int (default=‘50’)", "name": "threshold (t)", "description": " The minimum number of finite elements required in a column to impute the data onto nan’s and inf’s values: [1, 99999] "}, {"type": "bool (default=’False’)", "name": "scale_small (s)", "description": " True if small values (range of the numeric variable is <= 1) are to be scaled values: [False, True] "}], "title": "Missing Values Imputed Documentation — DataRobot Model Documentation", "links": [], "references": [{"url": "http://academic.uprm.edu/~eacuna/IFCS04r.pdf", "name": "[R56]Acuna, Edgar, and Caroline Rodriguez. “The treatment of missing values and its effect on classifier accuracy.” Classification, Clustering, and Data Mining Applications. Springer Berlin Heidelberg, 2004. 639-647. "}, {"url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.36.7991&rep=rep1&type=pdf", "name": "[R57]Feelders, Ad. “Handling missing data in trees: Surrogate splits or statistical imputation?.” Principles of Data Mining and Knowledge Discovery. Springer Berlin Heidelberg, 1999. 329-334. "}]}]